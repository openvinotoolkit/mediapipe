diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/build_desktop_examples.sh mediapipe/build_desktop_examples.sh
--- ../mediapipe/build_desktop_examples.sh	2023-10-12 11:40:46.927912448 +0200
+++ mediapipe/build_desktop_examples.sh	2023-10-11 16:18:55.901688474 +0200
@@ -34,9 +34,10 @@
 out_dir="."
 build_only=false
 run_only=false
+one_target=false
 app_dir="mediapipe/examples/desktop"
 bin_dir="bazel-bin"
-declare -a default_bazel_flags=(build -c opt --define MEDIAPIPE_DISABLE_GPU=1)
+declare -a default_bazel_flags=(build -c opt --define MEDIAPIPE_DISABLE_GPU=1 --define=MEDIAPIPE_DISABLE=1 --define=PYTHON_DISABLE=1 --cxxopt=-DPYTHON_DISABLE=1 --cxxopt=-DMEDIAPIPE_DISABLE=1)
 
 while [[ -n $1 ]]; do
   case $1 in
@@ -50,6 +51,12 @@
     -r)
       run_only=true
       ;;
+    -t)
+      one_target=true
+      shift
+      one_target_name=$1
+      echo "one_target_name: $one_target_name"
+      ;;
     *)
       echo "Unsupported input argument $1."
       exit 1
@@ -67,26 +74,52 @@
 for app in ${apps}; do
   if [[ -d "${app}" ]]; then
     target_name=${app##*/}
-    if [[ "${target_name}" == "autoflip" ||
-          "${target_name}" == "hello_world" ||
-          "${target_name}" == "media_sequence" ||
-          "${target_name}" == "object_detection_3d" ||
-          "${target_name}" == "template_matching" ||
-          "${target_name}" == "youtube8m" ]]; then
-      continue
+    if [[ "${target_name}" == "hello_world" ||
+          "${target_name}" == "hello_ovms" ]]; then
+      target="${app}:${target_name}"
+    elif [[ "${target_name}" == "media_sequence" ]]; then
+        target="${app}:${target_name}_demo"
+        echo "Skipping target ${target}"
+        continue
+    elif [[ "${target_name}" == "autoflip" ]]; then
+        target="${app}:run_${target_name}"
+    elif [[ "${target_name}" == "object_detection_3d" ]]; then
+        target="${app}:objectron_cpu"
+    elif [[ "${target_name}" == "object_detection" ]]; then
+        target="${app}:${target_name}_ovms"
+    elif [[ "${target_name}" == "template_matching" ]]; then
+        target="${app}:template_matching_tflite"
+    elif [[ "${target_name}" == "youtube8m" ]]; then
+        target="${app}:extract_yt8m_features"
+        echo "Skipping target ${target}"
+        continue
+    else
+      target="${app}:${target_name}_cpu"
     fi
-    target="${app}:${target_name}_cpu"
 
-    echo "=== Target: ${target}"
+    echo "target_name:${target_name}"
+    echo "target:${target}"
+    if [[ $one_target == true ]]; then
+      if [[ "${target_name}" == "${one_target_name}" ]]; then
+        bazel_flags=("${default_bazel_flags[@]}")
+        bazel_flags+=(${target})
+        echo "BUILD COMMAND:bazelisk ${bazel_flags[@]}"
+        bazelisk "${bazel_flags[@]}"
+        exit 0
+      else
+        continue
+      fi
+    fi
 
     if [[ $run_only == false ]]; then
       bazel_flags=("${default_bazel_flags[@]}")
       bazel_flags+=(${target})
 
       bazelisk "${bazel_flags[@]}"
-      cp -f "${bin_dir}/${app}/"*"_cpu" "${out_dir}"
     fi
+
     if [[ $build_only == false ]]; then
+      cp -f "${bin_dir}/${app}/"*"" "${out_dir}"
       if  [[ ${target_name} == "object_tracking" ]]; then
         graph_name="tracking/object_detection_tracking"
       elif [[ ${target_name} == "upper_body_pose_tracking" ]]; then
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/calculators/core/BUILD mediapipe/mediapipe/calculators/core/BUILD
--- ../mediapipe/mediapipe/calculators/core/BUILD	2023-10-12 11:41:03.673636584 +0200
+++ mediapipe/mediapipe/calculators/core/BUILD	2023-10-11 16:18:55.909689299 +0200
@@ -1380,4 +1380,4 @@
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/types:optional",
     ],
-)
+)
\ No newline at end of file
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/demo_run_graph_main.cc mediapipe/mediapipe/examples/desktop/demo_run_graph_main.cc
--- ../mediapipe/mediapipe/examples/desktop/demo_run_graph_main.cc	2023-10-12 11:41:03.693638644 +0200
+++ mediapipe/mediapipe/examples/desktop/demo_run_graph_main.cc	2023-11-09 10:53:26.501167566 +0100
@@ -14,6 +14,7 @@
 //
 // An example of sending OpenCV webcam frames into a MediaPipe graph.
 #include <cstdlib>
+#include <chrono>
 
 #include "absl/flags/flag.h"
 #include "absl/flags/parse.h"
@@ -84,6 +85,9 @@
 
   LOG(INFO) << "Start grabbing and processing frames.";
   bool grab_frames = true;
+  int count_frames = 0;
+  auto begin = std::chrono::high_resolution_clock::now();
+
   while (grab_frames) {
     // Capture opencv camera or video frame.
     cv::Mat camera_frame_raw;
@@ -96,6 +100,7 @@
       LOG(INFO) << "Empty frame, end of video reached.";
       break;
     }
+    count_frames+=1;
     cv::Mat camera_frame;
     cv::cvtColor(camera_frame_raw, camera_frame, cv::COLOR_BGR2RGB);
     if (!load_video) {
@@ -140,7 +145,12 @@
       if (pressed_key >= 0 && pressed_key != 255) grab_frames = false;
     }
   }
+  auto duration = std::chrono::duration_cast<std::chrono::microseconds>(std::chrono::high_resolution_clock::now() - begin);
+  auto totalTime = duration.count();
+  float avgFps = (1000000 * (float)(count_frames) / (float)totalTime);
+  float avgLatencyms = 1000 / avgFps;
 
+  LOG(INFO) << "Frames:" << count_frames << ", Duration [ms]:" << totalTime / 1000 << ", FPS:" << avgFps << ", Avg latency [ms]:" << avgLatencyms;   
   LOG(INFO) << "Shutting down.";
   if (writer.isOpened()) writer.release();
   MP_RETURN_IF_ERROR(graph.CloseInputStream(kInputStream));
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/face_detection/BUILD mediapipe/mediapipe/examples/desktop/face_detection/BUILD
--- ../mediapipe/mediapipe/examples/desktop/face_detection/BUILD	2023-01-30 17:52:09.578456772 +0100
+++ mediapipe/mediapipe/examples/desktop/face_detection/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -29,6 +29,7 @@
     name = "face_detection_cpu",
     data = ["//mediapipe/modules/face_detection:face_detection_short_range.tflite"],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/face_detection:desktop_live_calculators",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/face_mesh/BUILD mediapipe/mediapipe/examples/desktop/face_mesh/BUILD
--- ../mediapipe/mediapipe/examples/desktop/face_mesh/BUILD	2023-01-30 17:52:09.578456772 +0100
+++ mediapipe/mediapipe/examples/desktop/face_mesh/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -29,6 +29,7 @@
     name = "face_mesh_cpu",
     data = ["//mediapipe/modules/face_landmark:face_landmark_with_attention.tflite"],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/face_mesh:desktop_live_calculators",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/hand_tracking/BUILD mediapipe/mediapipe/examples/desktop/hand_tracking/BUILD
--- ../mediapipe/mediapipe/examples/desktop/hand_tracking/BUILD	2023-01-30 17:52:09.578456772 +0100
+++ mediapipe/mediapipe/examples/desktop/hand_tracking/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -35,6 +35,7 @@
         "//mediapipe/modules/palm_detection:palm_detection_full.tflite",
     ],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/hand_tracking:desktop_tflite_calculators",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/hello_world/BUILD mediapipe/mediapipe/examples/desktop/hello_world/BUILD
--- ../mediapipe/mediapipe/examples/desktop/hello_world/BUILD	2023-10-12 11:41:03.693638644 +0200
+++ mediapipe/mediapipe/examples/desktop/hello_world/BUILD	2023-07-14 14:26:13.864943372 +0200
@@ -27,3 +27,4 @@
         "//mediapipe/framework/port:status",
     ],
 )
+
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/holistic_tracking/BUILD mediapipe/mediapipe/examples/desktop/holistic_tracking/BUILD
--- ../mediapipe/mediapipe/examples/desktop/holistic_tracking/BUILD	2023-01-30 17:52:09.578456772 +0100
+++ mediapipe/mediapipe/examples/desktop/holistic_tracking/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -24,8 +24,10 @@
         "//mediapipe/modules/holistic_landmark:hand_recrop.tflite",
         "//mediapipe/modules/pose_detection:pose_detection.tflite",
         "//mediapipe/modules/pose_landmark:pose_landmark_full.tflite",
+        "//mediapipe/modules/face_landmark:face_landmark_with_attention.tflite"
     ],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/holistic_tracking:holistic_tracking_cpu_graph_deps",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/iris_tracking/BUILD mediapipe/mediapipe/examples/desktop/iris_tracking/BUILD
--- ../mediapipe/mediapipe/examples/desktop/iris_tracking/BUILD	2023-10-12 11:41:03.693638644 +0200
+++ mediapipe/mediapipe/examples/desktop/iris_tracking/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -49,6 +49,7 @@
     name = "iris_tracking_cpu",
     data = ["//mediapipe/modules/iris_landmark:iris_landmark.tflite"],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/iris_tracking:iris_tracking_cpu_deps",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/object_detection/BUILD mediapipe/mediapipe/examples/desktop/object_detection/BUILD
--- ../mediapipe/mediapipe/examples/desktop/object_detection/BUILD	2023-01-30 17:52:09.582457179 +0100
+++ mediapipe/mediapipe/examples/desktop/object_detection/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -53,3 +53,4 @@
         "//mediapipe/graphs/object_detection:desktop_tflite_calculators",
     ],
 )
+
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/desktop/pose_tracking/BUILD mediapipe/mediapipe/examples/desktop/pose_tracking/BUILD
--- ../mediapipe/mediapipe/examples/desktop/pose_tracking/BUILD	2023-01-30 17:52:09.590457991 +0100
+++ mediapipe/mediapipe/examples/desktop/pose_tracking/BUILD	2023-12-04 12:37:47.853890762 +0100
@@ -23,6 +23,7 @@
         "//mediapipe/modules/pose_landmark:pose_landmark_full.tflite",
     ],
     deps = [
+        "@ovms//src:ovms_lib",
         "//mediapipe/examples/desktop:demo_run_graph_main",
         "//mediapipe/graphs/pose_tracking:pose_tracking_cpu_deps",
     ],
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/python/ovms_face_detection.py mediapipe/mediapipe/examples/python/ovms_face_detection.py
--- ../mediapipe/mediapipe/examples/python/ovms_face_detection.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/examples/python/ovms_face_detection.py	2023-11-27 13:34:35.515906767 +0100
@@ -0,0 +1,80 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import argparse
+import cv2
+import mediapipe as mp
+import time
+import numpy as np
+import threading
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--input_video_path', required=False, default="/mediapipe/video.mp4", type=str, help='Camera ID number or path to a video file')
+parser.add_argument('--output_video_path', required=False, default="face_output.mp4", type=str, help='Output path to a video file')
+args = parser.parse_args()
+
+try:
+    source = int(args.input_video_path)
+except ValueError:
+    source = args.input_video_path
+
+output = args.output_video_path
+
+cap = cv2.VideoCapture(source)
+out = cv2.VideoWriter()
+fps = cap.get(cv2.CAP_PROP_FPS)
+height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+print("Input file FPS - %s width - %s height - %s" % (fps, width, height ))
+out.open(output, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, (width, height))
+if not out.isOpened():
+    print("Output file open error: " + str(out.isOpened()))
+    exit()
+
+print("Output file opened for writing: " + str(out.isOpened()))
+
+def grab_frame(cap):
+    success, frame = cap.read()
+    if not success:
+        print("[WARNING] No Input frame")
+        return None
+    return frame
+
+input_frame = grab_frame(cap)
+if  input_frame is None:
+    print("[ERROR] Check camera or file input...")
+    exit(-1)
+
+ovms_face_detection = mp.solutions.ovms_face_detection
+with ovms_face_detection.OvmsFaceDetection() as ovms_face_detection:
+    while input_frame is not None:      
+        result = ovms_face_detection.process(input_frame)
+        if result is None:
+            output_frame = np.array(input_frame, copy=True)
+        else:
+            # output_video is the output_stream name from the graph
+            output_frame = result.output_video
+
+        out.write(output_frame)
+
+        input_frame = grab_frame(cap)
+
+        if cv2.waitKey(1) & 0xFF == ord('q'):
+            break
+
+# When everything done, release the capture
+cap.release()
+out.release()
+print(f"Finished.")
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/python/ovms_holistic_tracking_multithread.py mediapipe/mediapipe/examples/python/ovms_holistic_tracking_multithread.py
--- ../mediapipe/mediapipe/examples/python/ovms_holistic_tracking_multithread.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/examples/python/ovms_holistic_tracking_multithread.py	2023-11-27 13:34:35.515906767 +0100
@@ -0,0 +1,178 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import argparse
+import cv2
+import mediapipe as mp
+import time
+import numpy as np
+import threading
+
+class RequestingThread(threading.Thread):
+    def __init__(self, index):
+        print(f"Initializing requesting thread index: {index}")
+        super().__init__()
+        self.index = index
+        self.input_frame = None
+        self.output_frame = None
+        self.predict_durations = []
+        self.input_ready_event = threading.Event()
+        self.output_ready_event = threading.Event()
+
+    def is_initialized(self):
+        return not (self.input_frame is None and self.output_frame is None)
+
+    def wait_for_input(self):
+        self.input_ready_event.wait()
+        self.input_ready_event.clear()
+
+    def wait_for_result(self):
+        self.output_ready_event.wait()
+        self.output_ready_event.clear()
+
+    def notify_input_ready(self):
+        self.input_ready_event.set()
+
+    def notify_output_ready(self):
+        self.output_ready_event.set()
+
+    def set_input(self, frame):
+        self.input_frame = frame
+        self.notify_input_ready()
+
+    def get_output(self):
+        return self.output_frame
+
+    def get_average_latency(self):
+        return np.average(np.array(self.predict_durations))
+
+    def run(self):
+        print(f"Launching requesting thread index: {self.index}")
+        global force_exit
+        ovms_holistic_tracking = mp.solutions.ovms_holistic_tracking
+        with ovms_holistic_tracking.OvmsHolisticTracking() as ovms_holistic_tracking:
+            while (True):
+                self.wait_for_input()
+                if force_exit:
+                        print("Detected exit signal...")
+                        break
+
+                predict_start_time = time.time()
+                result = ovms_holistic_tracking.process(self.input_frame)
+                
+                predict_duration = time.time() - predict_start_time
+                predict_duration *= 1000
+                self.predict_durations.append(predict_duration)
+
+                if result is None:
+                    self.output_frame = np.array(self.input_frame, copy=True)
+                else:
+                    # output_video is the output_stream name from the graph
+                    self.output_frame = result.output_video
+
+                self.notify_output_ready()
+                print(f"Stopping requesting thread index: {self.index}")
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--num_threads', required=False, default=4, type=int, help='Number of threads for parallel service requesting')
+parser.add_argument('--input_video_path', required=False, default="/mediapipe/video.mp4", type=str, help='Camera ID number or path to a video file')
+parser.add_argument('--output_video_path', required=False, default="holistic_output.mp4", type=str, help='Output path to a video file')
+args = parser.parse_args()
+
+try:
+    source = int(args.input_video_path)
+except ValueError:
+    source = args.input_video_path
+
+output = args.output_video_path
+
+cap = cv2.VideoCapture(source)
+out = cv2.VideoWriter()
+fps = cap.get(cv2.CAP_PROP_FPS)
+height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+print("Input file FPS - %s width - %s height - %s" % (fps, width, height ))
+out.open(output, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, (width, height))
+if not out.isOpened():
+    print("Output file open error: " + str(out.isOpened()))
+    exit()
+
+print("Output file opened for writing: " + str(out.isOpened()))
+force_exit = False
+    
+threads = [RequestingThread(i) for i in range(args.num_threads)]
+
+for thread in threads:
+    thread.start()
+
+def finish():
+    global force_exit
+    force_exit = True
+    for thread in threads:
+        thread.notify_input_ready()
+        thread.join()
+
+def grab_frame(cap):
+    success, frame = cap.read()
+    if not success:
+        print("[WARNING] No Input frame")
+        finish()
+        return None
+    return frame
+
+thread_id = 0
+frames_processed = 0
+last_display_time = time.time()
+app_start_time = time.time()
+
+if grab_frame(cap) is None:
+    print("[ERROR] Check camera input...")
+    force_exit = True
+
+while not force_exit:
+    if not threads[thread_id].is_initialized():
+        threads[thread_id].set_input(grab_frame(cap))
+        thread_id = (thread_id + 1) % args.num_threads
+        continue
+
+    threads[thread_id].wait_for_result()
+    avg_latency_for_thread = threads[thread_id].get_average_latency()
+    frame_to_display = threads[thread_id].get_output()
+    threads[thread_id].set_input(grab_frame(cap))
+
+    out.write(frame_to_display)
+    now = time.time()
+    time_since_last_display = now - last_display_time
+    last_display_time = now
+
+    frames_processed += 1
+
+    current_fps = 1 / (time_since_last_display if time_since_last_display > 0 else 1)
+    avg_fps = 1 / ((now - app_start_time) / frames_processed)
+    
+    print(f"ThreadID: {thread_id:3}; Current FPS: {current_fps:8.2f}; Average FPS: {avg_fps:8.2f}; Average latency: {avg_latency_for_thread:8.2f}ms")
+    if cv2.waitKey(1) & 0xFF == ord('q'):
+        finish()
+        break
+
+    thread_id = (thread_id + 1) % args.num_threads
+
+total_time = time.time() - app_start_time
+print(f"Total processing time: {total_time:8.2f}s")
+finish()
+# When everything done, release the capture
+cap.release()
+out.release()
+print(f"Finished.")
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/python/ovms_holistic_tracking.py mediapipe/mediapipe/examples/python/ovms_holistic_tracking.py
--- ../mediapipe/mediapipe/examples/python/ovms_holistic_tracking.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/examples/python/ovms_holistic_tracking.py	2023-11-27 13:34:35.515906767 +0100
@@ -0,0 +1,78 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import argparse
+import cv2
+import mediapipe as mp
+import numpy as np
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--input_video_path', required=False, default="/mediapipe/video.mp4", type=str, help='Camera ID number or path to a video file')
+parser.add_argument('--output_video_path', required=False, default="holistic_output.mp4", type=str, help='Output path to a video file')
+args = parser.parse_args()
+
+try:
+    source = int(args.input_video_path)
+except ValueError:
+    source = args.input_video_path
+
+output = args.output_video_path
+
+cap = cv2.VideoCapture(source)
+out = cv2.VideoWriter()
+fps = cap.get(cv2.CAP_PROP_FPS)
+height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+print("Input file FPS - %s width - %s height - %s" % (fps, width, height ))
+out.open(output, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, (width, height))
+if not out.isOpened():
+    print("Output file open error: " + str(out.isOpened()))
+    exit()
+
+print("Output file opened for writing: " + str(out.isOpened()))
+
+def grab_frame(cap):
+    success, frame = cap.read()
+    if not success:
+        print("[WARNING] No Input frame")
+        return None
+    return frame
+
+input_frame = grab_frame(cap)
+if  input_frame is None:
+    print("[ERROR] Check camera or file input...")
+    exit(-1)
+
+ovms_holistic_tracking = mp.solutions.ovms_holistic_tracking
+with ovms_holistic_tracking.OvmsHolisticTracking() as ovms_holistic_tracking: 
+    while input_frame is not None:      
+        result = ovms_holistic_tracking.process(input_frame)
+        if result is None:
+            output_frame = np.array(input_frame, copy=True)
+        else:
+            # output_video is the output_stream name from the graph
+            output_frame = result.output_video
+
+        out.write(output_frame)
+
+        input_frame = grab_frame(cap)
+
+        if cv2.waitKey(1) & 0xFF == ord('q'):
+            break
+
+# When everything done, release the capture
+cap.release()
+out.release()
+print(f"Finished.")
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/examples/python/ovms_object_detection.py mediapipe/mediapipe/examples/python/ovms_object_detection.py
--- ../mediapipe/mediapipe/examples/python/ovms_object_detection.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/examples/python/ovms_object_detection.py	2023-11-27 13:34:35.515906767 +0100
@@ -0,0 +1,31 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import mediapipe as mp
+import argparse
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--input_video_path', required=False, default="/mediapipe/mediapipe/examples/desktop/object_detection/test_video.mp4", type=str, help='Path to a video file')
+parser.add_argument('--output_video_path', required=False, default="/mediapipe/object_output.mp4", type=str, help='Output path to a video file')
+args = parser.parse_args()
+
+source = args.input_video_path
+output = args.output_video_path
+
+ovms_object_detection = mp.solutions.ovms_object_detection
+with ovms_object_detection.OvmsObjectDetection(side_inputs=
+        {'input_video_path':source,
+         'output_video_path':output}) as ovms_object_detection:
+        results = ovms_object_detection.process()
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/framework/BUILD mediapipe/mediapipe/framework/BUILD
--- ../mediapipe/mediapipe/framework/BUILD	2023-10-12 11:41:03.693638644 +0200
+++ mediapipe/mediapipe/framework/BUILD	2023-10-11 16:18:55.929691362 +0200
@@ -18,7 +18,7 @@
 
 licenses(["notice"])
 
-package(default_visibility = ["//visibility:private"])
+package(default_visibility = ["//visibility:public"])
 
 # The MediaPipe internal package group. No mediapipe users should be added to this group.
 package_group(
@@ -305,9 +305,6 @@
         "calculator_graph.h",
         "scheduler.h",
     ],
-    visibility = [
-        ":mediapipe_internal",
-    ],
     deps = [
         ":calculator_base",
         ":calculator_cc_proto",
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/framework/deps/file_helpers.cc mediapipe/mediapipe/framework/deps/file_helpers.cc
--- ../mediapipe/mediapipe/framework/deps/file_helpers.cc	2023-10-12 11:41:03.697639056 +0200
+++ mediapipe/mediapipe/framework/deps/file_helpers.cc	2023-09-27 16:56:57.362091001 +0200
@@ -151,6 +151,7 @@
     char buf[4096];
     size_t ret = fread(buf, 1, 4096, fp);
     if (ret == 0 && ferror(fp)) {
+      fclose(fp);
       return mediapipe::InternalErrorBuilder(MEDIAPIPE_LOC)
              << "Error while reading file: " << file_name;
     }
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/framework/formats/tensor.h mediapipe/mediapipe/framework/formats/tensor.h
--- ../mediapipe/mediapipe/framework/formats/tensor.h	2023-10-12 11:41:03.701639468 +0200
+++ mediapipe/mediapipe/framework/formats/tensor.h	2023-10-11 16:18:55.937692188 +0200
@@ -330,6 +330,7 @@
       case ElementType::kBool:
         return sizeof(bool);
     }
+    return 0; // making compiler happy
   }
   int bytes() const { return shape_.num_elements() * element_size(); }
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/graphs/object_detection/BUILD mediapipe/mediapipe/graphs/object_detection/BUILD
--- ../mediapipe/mediapipe/graphs/object_detection/BUILD	2023-01-30 17:52:09.630462054 +0100
+++ mediapipe/mediapipe/graphs/object_detection/BUILD	2023-12-04 11:08:53.700738381 +0100
@@ -53,6 +53,8 @@
     ],
 )
 
+# TODO for now we use TF Tensor as input so we may want to replace those in the end to pure OV::Tensor converters
+
 cc_library(
     name = "desktop_tflite_calculators",
     deps = [
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/__init__.py mediapipe/mediapipe/__init__.py
--- ../mediapipe/mediapipe/__init__.py	2023-11-27 15:50:32.069992170 +0100
+++ mediapipe/mediapipe/__init__.py	2023-11-20 11:18:25.835222014 +0100
@@ -1,13 +1,13 @@
-# Copyright 2019 - 2022 The MediaPipe Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+
+from mediapipe.python import *
+import mediapipe.python.solutions as solutions 
+import mediapipe.tasks.python as tasks
+
+
+del framework
+del gpu
+del modules
+del python
+del mediapipe
+del util
+__version__ = '1.0'
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/models/BUILD mediapipe/mediapipe/models/BUILD
--- ../mediapipe/mediapipe/models/BUILD	2023-01-30 17:52:09.646463679 +0100
+++ mediapipe/mediapipe/models/BUILD	2023-12-01 17:31:24.008877899 +0100
@@ -40,3 +40,4 @@
         "ssdlite_object_detection_labelmap.txt",
     ],
 )
+
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/face_detection/BUILD mediapipe/mediapipe/modules/face_detection/BUILD
--- ../mediapipe/mediapipe/modules/face_detection/BUILD	2023-01-30 17:52:09.646463679 +0100
+++ mediapipe/mediapipe/modules/face_detection/BUILD	2023-12-04 13:23:47.827394479 +0100
@@ -111,6 +111,7 @@
         "//mediapipe/calculators/util:detection_projection_calculator",
         "//mediapipe/calculators/util:non_max_suppression_calculator",
         "//mediapipe/calculators/util:to_image_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/face_detection/face_detection.pbtxt mediapipe/mediapipe/modules/face_detection/face_detection.pbtxt
--- ../mediapipe/mediapipe/modules/face_detection/face_detection.pbtxt	2023-01-30 17:52:09.646463679 +0100
+++ mediapipe/mediapipe/modules/face_detection/face_detection.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -62,15 +62,49 @@
 # Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
 # vector of tensors representing, for instance, detection boxes/keypoints and
 # scores.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_stream: "TENSORS:input_tensors"
+#  output_stream: "TENSORS:detection_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {}
+#  }
+#  option_value: "delegate:options/delegate"
+#  option_value: "model_path:options/model_path"
+#}
 node {
-  calculator: "InferenceCalculator"
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "face_detection"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:input_tensors"
   output_stream: "TENSORS:detection_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {}
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["regressors", "classificators"]
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "regressors"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "classificators"
+          }
+        }
   }
-  option_value: "delegate:options/delegate"
-  option_value: "model_path:options/model_path"
 }
 
 # Detection tensors. (std::vector<Tensor>)
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/face_landmark/BUILD mediapipe/mediapipe/modules/face_landmark/BUILD
--- ../mediapipe/mediapipe/modules/face_landmark/BUILD	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/face_landmark/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -43,6 +43,7 @@
         "//mediapipe/calculators/util:landmark_projection_calculator",
         "//mediapipe/calculators/util:thresholding_calculator",
         "//mediapipe/framework/tool:switch_container",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/face_landmark/face_landmark_cpu.pbtxt mediapipe/mediapipe/modules/face_landmark/face_landmark_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/face_landmark/face_landmark_cpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/face_landmark/face_landmark_cpu.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -77,19 +77,56 @@
 # Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
 # vector of tensors representing, for instance, detection boxes/keypoints and
 # scores.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_stream: "TENSORS:input_tensors"
+#  input_side_packet: "MODEL:model"
+#  input_side_packet: "OP_RESOLVER:op_resolver"
+#  output_stream: "TENSORS:output_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {
+#      delegate { xnnpack {} }
+#    }
+#  }
+#}
 node {
-  calculator: "InferenceCalculator"
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      #servable_name: "face_landmark_with_attention" # could be enabled when we have IR model
+      servable_name: "face_landmark"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:input_tensors"
-  input_side_packet: "MODEL:model"
-  input_side_packet: "OP_RESOLVER:op_resolver"
   output_stream: "TENSORS:output_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {
-      delegate { xnnpack {} }
-    }
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["conv2d_21", "conv2d_31"]
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input_1"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "conv2d_21"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "conv2d_31"
+          }
+        }
   }
 }
-
+#Input name: input_1; mapping_name: input_1; shape: (1,192,192,3); precision: FP32; layout: N...
+#Output name: conv2d_21; mapping_name: conv2d_21; shape: (1,1,1,1404); precision: FP32; layout: N...
+#Output name: conv2d_31; mapping_name: conv2d_31; shape: (1,1,1,1); precision: FP32; layout: N.
 # Splits a vector of tensors into landmark tensors and face flag tensor.
 node {
   calculator: "SwitchContainer"
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/face_landmark/face_landmarks_model_loader.pbtxt mediapipe/mediapipe/modules/face_landmark/face_landmarks_model_loader.pbtxt
--- ../mediapipe/mediapipe/modules/face_landmark/face_landmarks_model_loader.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/face_landmark/face_landmarks_model_loader.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -44,15 +44,15 @@
 }
 
 # Loads the file in the specified path into a blob.
-node {
-  calculator: "LocalFileContentsCalculator"
-  input_side_packet: "FILE_PATH:model_path"
-  output_side_packet: "CONTENTS:model_blob"
-}
+#node {
+#  calculator: "LocalFileContentsCalculator"
+#  input_side_packet: "FILE_PATH:model_path"
+#  output_side_packet: "CONTENTS:model_blob"
+#}
 
 # Converts the input blob into a TF Lite model.
-node {
-  calculator: "TfLiteModelCalculator"
-  input_side_packet: "MODEL_BLOB:model_blob"
-  output_side_packet: "MODEL:model"
-}
+#node {
+#  calculator: "TfLiteModelCalculator"
+#  input_side_packet: "MODEL_BLOB:model_blob"
+#  output_side_packet: "MODEL:model"
+#}
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/hand_landmark/BUILD mediapipe/mediapipe/modules/hand_landmark/BUILD
--- ../mediapipe/mediapipe/modules/hand_landmark/BUILD	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/hand_landmark/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -61,6 +61,7 @@
         "//mediapipe/calculators/util:landmark_projection_calculator",
         "//mediapipe/calculators/util:thresholding_calculator",
         "//mediapipe/calculators/util:world_landmark_projection_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/hand_landmark/hand_landmark_cpu.pbtxt mediapipe/mediapipe/modules/hand_landmark/hand_landmark_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/hand_landmark/hand_landmark_cpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/hand_landmark/hand_landmark_cpu.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -56,28 +56,63 @@
 }
 
 # Loads the hand landmark TF Lite model.
-node {
-  calculator: "HandLandmarkModelLoader"
-  input_side_packet: "MODEL_COMPLEXITY:model_complexity"
-  output_side_packet: "MODEL:model"
-}
+#node {
+#  calculator: "HandLandmarkModelLoader"
+#  input_side_packet: "MODEL_COMPLEXITY:model_complexity"
+#  output_side_packet: "MODEL:model"
+#}
 
 # Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
 # vector of tensors representing, for instance, detection boxes/keypoints and
 # scores.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_side_packet: "MODEL:model"
+#  input_stream: "TENSORS:input_tensor"
+#  output_stream: "TENSORS:output_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {
+#      delegate {
+#        xnnpack {}
+#      }
+#    }
+#  }
+#}
+node {
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "hand_landmark_full"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
 node {
-  calculator: "InferenceCalculator"
-  input_side_packet: "MODEL:model"
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:input_tensor"
   output_stream: "TENSORS:output_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {
-      delegate {
-        xnnpack {}
-      }
-    }
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["Identity","Identity_1","Identity_2","Identity_3"]
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input_1"
+          }
+          tag_to_output_tensor_names { # not tagging other outputs due to hardcode
+            key: "TENSORS"
+            value: "Identity"
+          }
+        }
   }
 }
+#Input name: input_1; mapping_name: input_1; shape: (1,224,224,3); precision: FP32; layout: N...
+#Output name: Identity; mapping_name: Identity; shape: (1,63); precision: FP32; layout: N...
+#Output name: Identity_1; mapping_name: Identity_1; shape: (1,1); precision: FP32; layout: N...
+#Output name: Identity_2; mapping_name: Identity_2; shape: (1,1); precision: FP32; layout: N...
+#Output name: Identity_3; mapping_name: Identity_3; shape: (1,63); precision: FP32; layout: N...
 
 # Splits a vector of tensors to multiple vectors according to the ranges
 # specified in option.
@@ -136,7 +171,7 @@
   options: {
     [mediapipe.TensorsToClassificationCalculatorOptions.ext] {
       top_k: 1
-      label_map_path: "mediapipe/modules/hand_landmark/handedness.txt"
+      label_map_path: "/mediapipe/mediapipe/modules/hand_landmark/handedness.txt"
       binary_classification: true
     }
   }
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/hand_landmark/hand_landmark_gpu.pbtxt mediapipe/mediapipe/modules/hand_landmark/hand_landmark_gpu.pbtxt
--- ../mediapipe/mediapipe/modules/hand_landmark/hand_landmark_gpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/hand_landmark/hand_landmark_gpu.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -130,7 +130,7 @@
   options: {
     [mediapipe.TensorsToClassificationCalculatorOptions.ext] {
       top_k: 1
-      label_map_path: "mediapipe/modules/hand_landmark/handedness.txt"
+      label_map_path: "/mediapipe/mediapipe/modules/hand_landmark/handedness.txt"
       binary_classification: true
     }
   }
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/holistic_landmark/BUILD mediapipe/mediapipe/modules/holistic_landmark/BUILD
--- ../mediapipe/mediapipe/modules/holistic_landmark/BUILD	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/holistic_landmark/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -171,6 +171,7 @@
         "//mediapipe/calculators/util:landmark_projection_calculator",
         "//mediapipe/calculators/util:landmarks_to_detection_calculator",
         "//mediapipe/calculators/util:rect_transformation_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/holistic_landmark/hand_recrop_by_roi_cpu.pbtxt mediapipe/mediapipe/modules/holistic_landmark/hand_recrop_by_roi_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/holistic_landmark/hand_recrop_by_roi_cpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/holistic_landmark/hand_recrop_by_roi_cpu.pbtxt	2023-09-27 16:56:57.362091001 +0200
@@ -36,17 +36,48 @@
 }
 
 # Predicts hand re-crop rectangle.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_stream: "TENSORS:initial_crop_tensor"
+#  output_stream: "TENSORS:landmark_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {
+#      model_path: "mediapipe/modules/holistic_landmark/hand_recrop.tflite"
+#      delegate { xnnpack {} }
+#    }
+#  }
+#}
 node {
-  calculator: "InferenceCalculator"
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "hand_recrop"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:initial_crop_tensor"
   output_stream: "TENSORS:landmark_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {
-      model_path: "mediapipe/modules/holistic_landmark/hand_recrop.tflite"
-      delegate { xnnpack {} }
-    }
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input_1"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "output_crop"
+          }
+        }
   }
 }
+#Input name: input_1; mapping_name: input_1; shape: (1,256,256,3); precision: FP32; layout: N...
+#Output name: output_crop; mapping_name: output_crop; shape: (1,1,1,4); precision: FP32; layout: N...
 
 # Decodes the landmark tensors into a vector of landmarks, where the landmark
 # coordinates are normalized by the size of the input image to the model. Two
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/iris_landmark/BUILD mediapipe/mediapipe/modules/iris_landmark/BUILD
--- ../mediapipe/mediapipe/modules/iris_landmark/BUILD	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/iris_landmark/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -41,6 +41,7 @@
         "//mediapipe/calculators/tflite:tflite_tensors_to_landmarks_calculator",
         "//mediapipe/calculators/util:landmark_letterbox_removal_calculator",
         "//mediapipe/calculators/util:landmark_projection_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/iris_landmark/iris_landmark_cpu.pbtxt mediapipe/mediapipe/modules/iris_landmark/iris_landmark_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/iris_landmark/iris_landmark_cpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/iris_landmark/iris_landmark_cpu.pbtxt	2023-11-14 15:33:04.774682257 +0100
@@ -75,18 +75,52 @@
 # Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
 # vector of tensors representing, for instance, detection boxes/keypoints and
 # scores.
+#node {
+#  calculator: "TfLiteInferenceCalculator"
+#  input_stream: "TENSORS:image_tensor"
+#  output_stream: "TENSORS:output_tensors"
+#  options: {
+#    [mediapipe.TfLiteInferenceCalculatorOptions.ext] {
+#      model_path: "mediapipe/modules/iris_landmark/iris_landmark.tflite"
+#      delegate { xnnpack {} }
+#    }
+#  }
+#}
 node {
-  calculator: "TfLiteInferenceCalculator"
-  input_stream: "TENSORS:image_tensor"
-  output_stream: "TENSORS:output_tensors"
-  options: {
-    [mediapipe.TfLiteInferenceCalculatorOptions.ext] {
-      model_path: "mediapipe/modules/iris_landmark/iris_landmark.tflite"
-      delegate { xnnpack {} }
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "iris_landmark"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
     }
   }
 }
 
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
+  input_stream: "TFLITE_TENSORS:image_tensor"
+  output_stream: "TFLITE_TENSORS:output_tensors"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["output_eyes_contours_and_brows", "output_iris"]
+          tag_to_input_tensor_names {
+            key: "TFLITE_TENSORS"
+            value: "input_1"
+          }
+          tag_to_output_tensor_names {
+            key: "TFLITE_TENSORS"
+            value: "output_eyes_contours_and_brows"
+          }
+          tag_to_output_tensor_names {
+            key: "TFLITE_TENSORS"
+            value: "output_iris"
+          }
+        }
+  }
+}
 # Splits a vector of TFLite tensors to multiple vectors according to the ranges
 # specified in option.
 node {
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/ovms_modules/BUILD mediapipe/mediapipe/modules/ovms_modules/BUILD
--- ../mediapipe/mediapipe/modules/ovms_modules/BUILD	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/modules/ovms_modules/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -0,0 +1,106 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+load(
+    "//mediapipe/framework/tool:mediapipe_graph.bzl",
+    "mediapipe_simple_subgraph",
+)
+load(
+    "//mediapipe/framework/tool:mediapipe_files.bzl",
+    "mediapipe_files",
+)
+load("//mediapipe/framework/port:build_config.bzl", "mediapipe_proto_library")
+load("//mediapipe/framework:mediapipe_cc_test.bzl", "mediapipe_cc_test")
+
+licenses(["notice"])
+
+package(default_visibility = ["//visibility:public"])
+
+mediapipe_simple_subgraph(
+    name = "object_detection_ovms",
+    graph = "object_detection_ovms.pbtxt",
+    register_as = "ObjectDetectionOvms",
+    deps = [
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
+        "@ovms//src:ovms_lib",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "holistic_tracking_ovms",
+    graph = "holistic_tracking_ovms.pbtxt",
+    register_as = "HolisticTrackingOvms",
+    deps = [
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
+        "@ovms//src:ovms_lib",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "face_detection_ovms",
+    graph = "face_detection_ovms.pbtxt",
+    register_as = "FaceDetectionOvms",
+    deps = [
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
+        "@ovms//src:ovms_lib",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "iris_tracking_ovms",
+    graph = "iris_tracking_ovms.pbtxt",
+    register_as = "IrisTrackingOvms",
+    deps = [
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
+        "@ovms//src:ovms_lib",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "pose_landmarks_to_render_data",
+    graph = "pose_landmarks_to_render_data.pbtxt",
+    register_as = "PoseLandmarksToRenderData",
+    deps = [
+        "//mediapipe/calculators/core:concatenate_vector_calculator",
+        "//mediapipe/calculators/core:split_proto_list_calculator",
+        "//mediapipe/calculators/util:landmarks_to_render_data_calculator",
+        "//mediapipe/calculators/util:rect_to_render_scale_calculator",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "pose_renderer_cpu",
+    graph = "pose_renderer_cpu.pbtxt",
+    register_as = "PoseRendererCpu",
+    deps = [
+        ":pose_landmarks_to_render_data",
+        "//mediapipe/calculators/image:image_properties_calculator",
+        "//mediapipe/calculators/image:recolor_calculator",
+        "//mediapipe/calculators/util:annotation_overlay_calculator",
+        "//mediapipe/calculators/util:detections_to_render_data_calculator",
+        "//mediapipe/calculators/util:rect_to_render_data_calculator",
+    ],
+)
+
+mediapipe_simple_subgraph(
+    name = "pose_tracking_ovms",
+    graph = "pose_tracking_ovms.pbtxt",
+    register_as = "PoseTrackingOvms",
+    deps = [
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators",
+        ":pose_renderer_cpu",
+        "@ovms//src:ovms_lib",
+    ],
+)
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/ovms_modules/face_detection_ovms.pbtxt mediapipe/mediapipe/modules/ovms_modules/face_detection_ovms.pbtxt
--- ../mediapipe/mediapipe/modules/ovms_modules/face_detection_ovms.pbtxt	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/modules/ovms_modules/face_detection_ovms.pbtxt	2023-11-27 13:34:35.519907171 +0100
@@ -0,0 +1,58 @@
+# MediaPipe graph that performs face mesh with TensorFlow Lite on CPU.
+
+# CPU buffer. (ImageFrame)
+input_stream: "input_video"
+
+# Output image with rendered results. (ImageFrame)
+output_stream: "output_video"
+# Detected faces. (std::vector<Detection>)
+output_stream: "face_detections"
+
+# Throttles the images flowing downstream for flow control. It passes through
+# the very first incoming image unaltered, and waits for downstream nodes
+# (calculators and subgraphs) in the graph to finish their tasks before it
+# passes through another image. All images that come in while waiting are
+# dropped, limiting the number of in-flight images in most part of the graph to
+# 1. This prevents the downstream nodes from queuing up incoming images and data
+# excessively, which leads to increased latency and memory usage, unwanted in
+# real-time mobile applications. It also eliminates unnecessarily computation,
+# e.g., the output produced by a node may get dropped downstream if the
+# subsequent nodes are still busy processing previous inputs.
+node {
+  calculator: "FlowLimiterCalculator"
+  input_stream: "input_video"
+  input_stream: "FINISHED:output_video"
+  input_stream_info: {
+    tag_index: "FINISHED"
+    back_edge: true
+  }
+  output_stream: "throttled_input_video"
+}
+
+# Subgraph that detects faces.
+node {
+  calculator: "FaceDetectionShortRangeCpu"
+  input_stream: "IMAGE:throttled_input_video"
+  output_stream: "DETECTIONS:face_detections"
+}
+
+# Converts the detections to drawing primitives for annotation overlay.
+node {
+  calculator: "DetectionsToRenderDataCalculator"
+  input_stream: "DETECTIONS:face_detections"
+  output_stream: "RENDER_DATA:render_data"
+  node_options: {
+    [type.googleapis.com/mediapipe.DetectionsToRenderDataCalculatorOptions] {
+      thickness: 4.0
+      color { r: 255 g: 0 b: 0 }
+    }
+  }
+}
+
+# Draws annotations and overlays them on top of the input images.
+node {
+  calculator: "AnnotationOverlayCalculator"
+  input_stream: "IMAGE:throttled_input_video"
+  input_stream: "render_data"
+  output_stream: "IMAGE:output_video"
+}
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/ovms_modules/holistic_tracking_ovms.pbtxt mediapipe/mediapipe/modules/ovms_modules/holistic_tracking_ovms.pbtxt
--- ../mediapipe/mediapipe/modules/ovms_modules/holistic_tracking_ovms.pbtxt	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/modules/ovms_modules/holistic_tracking_ovms.pbtxt	2023-11-27 13:34:35.519907171 +0100
@@ -0,0 +1,75 @@
+# Tracks and renders pose + hands + face landmarks.
+
+# CPU image. (ImageFrame)
+input_stream: "input_video"
+
+# CPU image with rendered results. (ImageFrame)
+output_stream: "output_video"
+
+# Throttles the images flowing downstream for flow control. It passes through
+# the very first incoming image unaltered, and waits for downstream nodes
+# (calculators and subgraphs) in the graph to finish their tasks before it
+# passes through another image. All images that come in while waiting are
+# dropped, limiting the number of in-flight images in most part of the graph to
+# 1. This prevents the downstream nodes from queuing up incoming images and data
+# excessively, which leads to increased latency and memory usage, unwanted in
+# real-time mobile applications. It also eliminates unnecessarily computation,
+# e.g., the output produced by a node may get dropped downstream if the
+# subsequent nodes are still busy processing previous inputs.
+node {
+  calculator: "FlowLimiterCalculator"
+  input_stream: "input_video"
+  input_stream: "FINISHED:output_video"
+  input_stream_info: {
+    tag_index: "FINISHED"
+    back_edge: true
+  }
+  output_stream: "throttled_input_video"
+  node_options: {
+    [type.googleapis.com/mediapipe.FlowLimiterCalculatorOptions] {
+      max_in_flight: 1
+      max_in_queue: 1
+      # Timeout is disabled (set to 0) as first frame processing can take more
+      # than 1 second.
+      in_flight_timeout: 0
+    }
+  }
+}
+
+node {
+  calculator: "HolisticLandmarkCpu"
+  input_stream: "IMAGE:throttled_input_video"
+  output_stream: "POSE_LANDMARKS:pose_landmarks"
+  output_stream: "POSE_ROI:pose_roi"
+  output_stream: "POSE_DETECTION:pose_detection"
+  output_stream: "FACE_LANDMARKS:face_landmarks"
+  output_stream: "LEFT_HAND_LANDMARKS:left_hand_landmarks"
+  output_stream: "RIGHT_HAND_LANDMARKS:right_hand_landmarks"
+}
+
+# Gets image size.
+node {
+  calculator: "ImagePropertiesCalculator"
+  input_stream: "IMAGE:throttled_input_video"
+  output_stream: "SIZE:image_size"
+}
+
+# Converts pose, hands and face landmarks to a render data vector.
+node {
+  calculator: "HolisticTrackingToRenderData"
+  input_stream: "IMAGE_SIZE:image_size"
+  input_stream: "POSE_LANDMARKS:pose_landmarks"
+  input_stream: "POSE_ROI:pose_roi"
+  input_stream: "LEFT_HAND_LANDMARKS:left_hand_landmarks"
+  input_stream: "RIGHT_HAND_LANDMARKS:right_hand_landmarks"
+  input_stream: "FACE_LANDMARKS:face_landmarks"
+  output_stream: "RENDER_DATA_VECTOR:render_data_vector"
+}
+
+# Draws annotations and overlays them on top of the input images.
+node {
+  calculator: "AnnotationOverlayCalculator"
+  input_stream: "IMAGE:throttled_input_video"
+  input_stream: "VECTOR:render_data_vector"
+  output_stream: "IMAGE:output_video"
+}
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/ovms_modules/object_detection_ovms.pbtxt mediapipe/mediapipe/modules/ovms_modules/object_detection_ovms.pbtxt
--- ../mediapipe/mediapipe/modules/ovms_modules/object_detection_ovms.pbtxt	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/modules/ovms_modules/object_detection_ovms.pbtxt	2023-11-13 13:27:08.114307281 +0100
@@ -0,0 +1,206 @@
+# MediaPipe graph that performs object detection on desktop with OpenVINO Model Server
+# on CPU.
+# Used in the example in
+# mediapipe/examples/desktop/object_detection:object_detection_openvino.
+
+# max_queue_size limits the number of packets enqueued on any input stream
+# by throttling inputs to the graph. This makes the graph only process one
+# frame per time.
+max_queue_size: 1
+
+# Decodes an input video file into images and a video header.
+node {
+  calculator: "OpenCvVideoDecoderCalculator"
+  input_side_packet: "INPUT_FILE_PATH:input_video_path"
+  output_stream: "VIDEO:input_video"
+  output_stream: "VIDEO_PRESTREAM:input_video_header"
+}
+
+# Transforms the input image on CPU to a 320x320 image. To scale the image, by
+# default it uses the STRETCH scale mode that maps the entire input image to the
+# entire transformed image. As a result, image aspect ratio may be changed and
+# objects in the image may be deformed (stretched or squeezed), but the object
+# detection model used in this graph is agnostic to that deformation.
+node: {
+  calculator: "ImageTransformationCalculator"
+  input_stream: "IMAGE:input_video"
+  output_stream: "IMAGE:transformed_input_video"
+  node_options: {
+    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions] {
+      output_width: 320
+      output_height: 320
+    }
+  }
+}
+
+# Converts the transformed input image on CPU into an image tensor as a
+# OpenVINOTensor. The zero_center option is set to true to normalize the
+# pixel values to [-1.f, 1.f] as opposed to [0.f, 1.f].
+node {
+  calculator: "OpenVINOConverterCalculator"
+  input_stream: "IMAGE:transformed_input_video"
+  output_stream: "TENSORS:image_tensor"
+  node_options: {
+    [type.googleapis.com/mediapipe.OpenVINOConverterCalculatorOptions] {
+      enable_normalization: true
+      zero_center: true
+    }
+  }
+}
+
+# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
+# vector of tensors representing, for instance, detection boxes/keypoints and
+# scores.
+node {
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "ssdlite_object_detection"  # servable name inside OVMS
+      servable_version: "1"
+      server_config: "/mediapipe/mediapipe/calculators/ovms/config.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
+  input_stream: "OVTENSORS:image_tensor"
+  output_stream: "OVTENSORS2:detection_tensors"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          input_order_list :["normalized_input_image_tensor"]
+          output_order_list :["raw_outputs/box_encodings","raw_outputs/class_predictions"]
+          tag_to_input_tensor_names {
+            key: "OVTENSORS"
+            value: "normalized_input_image_tensor"
+          }
+          tag_to_output_tensor_names {
+            key: "OVTENSORS1"
+            value: "raw_outputs/box_encodings"
+          }
+          tag_to_output_tensor_names {
+            key: "OVTENSORS2"
+            value: "raw_outputs/class_predictions"
+          }
+        }
+  }
+}
+
+# Generates a single side packet containing a vector of SSD anchors based on
+# the specification in the options.
+node {
+  calculator: "SsdAnchorsCalculator"
+  output_side_packet: "anchors"
+  node_options: {
+    [type.googleapis.com/mediapipe.SsdAnchorsCalculatorOptions] {
+      num_layers: 6
+      min_scale: 0.2
+      max_scale: 0.95
+      input_size_height: 320
+      input_size_width: 320
+      anchor_offset_x: 0.5
+      anchor_offset_y: 0.5
+      strides: 16
+      strides: 32
+      strides: 64
+      strides: 128
+      strides: 256
+      strides: 512
+      aspect_ratios: 1.0
+      aspect_ratios: 2.0
+      aspect_ratios: 0.5
+      aspect_ratios: 3.0
+      aspect_ratios: 0.3333
+      reduce_boxes_in_lowest_layer: true
+    }
+  }
+}
+
+# Decodes the detection tensors generated by the TensorFlow Lite model, based on
+# the SSD anchors and the specification in the options, into a vector of
+# detections. Each detection describes a detected object.
+node {
+  calculator: "OpenVINOTensorsToDetectionsCalculator"
+  input_stream: "TENSORS:detection_tensors"
+  input_side_packet: "ANCHORS:anchors"
+  output_stream: "DETECTIONS:detections"
+  node_options: {
+    [type.googleapis.com/mediapipe.OpenVINOTensorsToDetectionsCalculatorOptions] {
+      num_classes: 91
+      num_boxes: 2034
+      num_coords: 4
+      ignore_classes: 0
+      apply_exponential_on_box_size: true
+
+      x_scale: 10.0
+      y_scale: 10.0
+      h_scale: 5.0
+      w_scale: 5.0
+    }
+  }
+}
+
+# Performs non-max suppression to remove excessive detections.
+node {
+  calculator: "NonMaxSuppressionCalculator"
+  input_stream: "detections"
+  output_stream: "filtered_detections"
+  node_options: {
+    [type.googleapis.com/mediapipe.NonMaxSuppressionCalculatorOptions] {
+      min_suppression_threshold: 0.4
+      min_score_threshold: 0.6
+      max_num_detections: 5
+      overlap_type: INTERSECTION_OVER_UNION
+    }
+  }
+}
+
+# Maps detection label IDs to the corresponding label text. The label map is
+# provided in the label_map_path option.
+node {
+  calculator: "DetectionLabelIdToTextCalculator"
+  input_stream: "filtered_detections"
+  output_stream: "output_detections"
+  node_options: {
+    [type.googleapis.com/mediapipe.DetectionLabelIdToTextCalculatorOptions] {
+      label_map_path: "/mediapipe/mediapipe/models/ssdlite_object_detection_labelmap.txt"
+    }
+  }
+}
+
+# Converts the detections to drawing primitives for annotation overlay.
+node {
+  calculator: "DetectionsToRenderDataCalculator"
+  input_stream: "DETECTIONS:output_detections"
+  output_stream: "RENDER_DATA:render_data"
+  node_options: {
+    [type.googleapis.com/mediapipe.DetectionsToRenderDataCalculatorOptions] {
+      thickness: 4.0
+      color { r: 255 g: 0 b: 0 }
+    }
+  }
+}
+
+# Draws annotations and overlays them on top of the input images.
+node {
+  calculator: "AnnotationOverlayCalculator"
+  input_stream: "IMAGE:input_video"
+  input_stream: "render_data"
+  output_stream: "IMAGE:output_video"
+}
+
+# Encodes the annotated images into a video file, adopting properties specified
+# in the input video header, e.g., video framerate.
+node {
+  calculator: "OpenCvVideoEncoderCalculator"
+  input_stream: "VIDEO:output_video"
+  input_stream: "VIDEO_PRESTREAM:input_video_header"
+  input_side_packet: "OUTPUT_FILE_PATH:output_video_path"
+  node_options: {
+    [type.googleapis.com/mediapipe.OpenCvVideoEncoderCalculatorOptions]: {
+      codec: "avc1"
+      video_format: "mp4"
+    }
+  }
+}
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/pose_detection/BUILD mediapipe/mediapipe/modules/pose_detection/BUILD
--- ../mediapipe/mediapipe/modules/pose_detection/BUILD	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/pose_detection/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -36,6 +36,7 @@
         "//mediapipe/calculators/tflite:ssd_anchors_calculator",
         "//mediapipe/calculators/util:detection_letterbox_removal_calculator",
         "//mediapipe/calculators/util:non_max_suppression_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt mediapipe/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt	2023-01-30 17:52:09.654464491 +0100
+++ mediapipe/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt	2023-09-27 16:56:57.366091411 +0200
@@ -65,19 +65,52 @@
 # Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
 # vector of tensors representing, for instance, detection boxes/keypoints and
 # scores.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_stream: "TENSORS:input_tensors"
+#  output_stream: "TENSORS:detection_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {
+#      model_path: "mediapipe/modules/pose_detection/pose_detection.tflite"
+#      delegate {
+#        xnnpack {}
+#      }
+#    }
+#  }
+#}
 node {
-  calculator: "InferenceCalculator"
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "pose_detection"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:input_tensors"
   output_stream: "TENSORS:detection_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {
-      model_path: "mediapipe/modules/pose_detection/pose_detection.tflite"
-      delegate {
-        xnnpack {}
-      }
-    }
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["Identity:0", "Identity_1:0"]
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input_1:0"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "Identity:0"
+          }
+        }
   }
 }
+#Input name: input_1:0; mapping_name: input_1:0; shape: (1,3,224,224); precision: FP32; layout: N...
+#Output name: Identity:0; mapping_name: Identity; shape: (1,2254,12); precision: FP32; layout: N...
+#Output name: Identity_1; mapping_name: Identity_1; shape: (1,2254,1); precision: FP32; layout: N...
 
 # Generates a single side packet containing a vector of SSD anchors based on
 # the specification in the options.
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/pose_landmark/BUILD mediapipe/mediapipe/modules/pose_landmark/BUILD
--- ../mediapipe/mediapipe/modules/pose_landmark/BUILD	2023-01-30 17:52:09.658464898 +0100
+++ mediapipe/mediapipe/modules/pose_landmark/BUILD	2023-12-04 13:23:44.919101975 +0100
@@ -62,6 +62,7 @@
         "//mediapipe/calculators/image:image_properties_calculator",
         "//mediapipe/calculators/tensor:image_to_tensor_calculator",
         "//mediapipe/calculators/tensor:inference_calculator",
+        "@//mediapipe/graphs/object_detection:desktop_ovms_calculators"
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt mediapipe/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt
--- ../mediapipe/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt	2023-01-30 17:52:09.658464898 +0100
+++ mediapipe/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt	2023-09-27 16:56:57.366091411 +0200
@@ -134,17 +134,53 @@
 }
 
 # Runs model inference on CPU.
+#node {
+#  calculator: "InferenceCalculator"
+#  input_side_packet: "MODEL:model"
+#  input_stream: "TENSORS:input_tensors"
+#  output_stream: "TENSORS:output_tensors"
+#  options: {
+#    [mediapipe.InferenceCalculatorOptions.ext] {
+#      delegate { xnnpack {} }
+#    }
+#  }
+#}
 node {
-  calculator: "InferenceCalculator"
-  input_side_packet: "MODEL:model"
+  calculator: "OpenVINOModelServerSessionCalculator"
+  output_side_packet: "SESSION:session"
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOModelServerSessionCalculatorOptions]: {
+      servable_name: "pose_landmark_full"
+      servable_version: "1"
+      server_config: "mediapipe/models/ovms/config_holistic.json"
+    }
+  }
+}
+node {
+  calculator: "OpenVINOInferenceCalculator"
+  input_side_packet: "SESSION:session"
   input_stream: "TENSORS:input_tensors"
   output_stream: "TENSORS:output_tensors"
-  options: {
-    [mediapipe.InferenceCalculatorOptions.ext] {
-      delegate { xnnpack {} }
-    }
+  node_options: {
+    [type.googleapis.com / mediapipe.OpenVINOInferenceCalculatorOptions]: {
+          output_order_list: ["Identity","Identity_1","Identity_2","Identity_3","Identity_4"]
+          tag_to_input_tensor_names {
+            key: "TENSORS"
+            value: "input_1"
+          }
+          tag_to_output_tensor_names {
+            key: "TENSORS"
+            value: "Identity"
+          }
+        }
   }
 }
+#Input name: input_1; mapping_name: input_1; shape: (1,256,256,3); precision: FP32; layout: N...
+#Output name: Identity; mapping_name: Identity; shape: (1,195); precision: FP32; layout: N...
+#Output name: Identity_1; mapping_name: Identity_1; shape: (1,1); precision: FP32; layout: N...
+#Output name: Identity_2; mapping_name: Identity_2; shape: (1,256,256,1); precision: FP32; layout: N...
+#Output name: Identity_3; mapping_name: Identity_3; shape: (1,64,64,39); precision: FP32; layout: N...
+#Output name: Identity_4; mapping_name: Identity_4; shape: (1,117); precision: FP32; layout: N...
 
 # Decodes the tensors into the corresponding landmark and segmentation mask
 # representation.
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/modules/pose_landmark/pose_landmark_model_loader.pbtxt mediapipe/mediapipe/modules/pose_landmark/pose_landmark_model_loader.pbtxt
--- ../mediapipe/mediapipe/modules/pose_landmark/pose_landmark_model_loader.pbtxt	2023-01-30 17:52:09.658464898 +0100
+++ mediapipe/mediapipe/modules/pose_landmark/pose_landmark_model_loader.pbtxt	2023-09-27 16:56:57.366091411 +0200
@@ -54,20 +54,20 @@
 }
 
 # Loads the file in the specified path into a blob.
-node {
-  calculator: "LocalFileContentsCalculator"
-  input_side_packet: "FILE_PATH:model_path"
-  output_side_packet: "CONTENTS:model_blob"
-  options: {
-    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
-      text_mode: false
-    }
-  }
-}
+#node {
+#  calculator: "LocalFileContentsCalculator"
+#  input_side_packet: "FILE_PATH:model_path"
+#  output_side_packet: "CONTENTS:model_blob"
+#  options: {
+#    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
+#      text_mode: false
+#    }
+#  }
+#}
 
 # Converts the input blob into a TF Lite model.
-node {
-  calculator: "TfLiteModelCalculator"
-  input_side_packet: "MODEL_BLOB:model_blob"
-  output_side_packet: "MODEL:model"
-}
+#node {
+#  calculator: "TfLiteModelCalculator"
+#  input_side_packet: "MODEL_BLOB:model_blob"
+#  output_side_packet: "MODEL:model"
+#}
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/python/BUILD mediapipe/mediapipe/python/BUILD
--- ../mediapipe/mediapipe/python/BUILD	2023-10-12 11:41:03.721641527 +0200
+++ mediapipe/mediapipe/python/BUILD	2023-12-05 14:32:15.401622393 +0100
@@ -58,6 +58,8 @@
         "//mediapipe/framework/formats:rect_registration",
         "//mediapipe/modules/objectron/calculators:annotation_registration",
         "//mediapipe/tasks/cc/vision/face_geometry/proto:face_geometry_registration",
+        # OVMS lib
+        "@ovms//src:ovms_lib",
     ],
 )
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/python/solutions/__init__.py mediapipe/mediapipe/python/solutions/__init__.py
--- ../mediapipe/mediapipe/python/solutions/__init__.py	2023-01-30 17:52:09.662465304 +0100
+++ mediapipe/mediapipe/python/solutions/__init__.py	2023-11-27 13:34:35.519907171 +0100
@@ -23,5 +23,8 @@
 import mediapipe.python.solutions.hands_connections
 import mediapipe.python.solutions.holistic
 import mediapipe.python.solutions.objectron
+import mediapipe.python.solutions.ovms_object_detection
+import mediapipe.python.solutions.ovms_face_detection
+import mediapipe.python.solutions.ovms_holistic_tracking
 import mediapipe.python.solutions.pose
 import mediapipe.python.solutions.selfie_segmentation
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/python/solutions/ovms_face_detection.py mediapipe/mediapipe/python/solutions/ovms_face_detection.py
--- ../mediapipe/mediapipe/python/solutions/ovms_face_detection.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/python/solutions/ovms_face_detection.py	2023-11-27 13:34:35.519907171 +0100
@@ -0,0 +1,38 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+import numpy as np
+from typing import NamedTuple
+
+from mediapipe.calculators.ovms import openvinoinferencecalculator_pb2
+from mediapipe.calculators.ovms import openvinomodelserversessioncalculator_pb2
+from mediapipe.python.solution_base import SolutionBase
+
+_FULL_GRAPH_FILE_PATH = 'mediapipe/modules/ovms_modules/face_detection_ovms.binarypb'
+
+class OvmsFaceDetection(SolutionBase):
+  """Ovms Face Detection.
+
+  Ovms Face Detection processes an input image frame returns output image frame
+  with detected objects.
+  """
+  def __init__(self):
+    """Initializes a Ovms Face Detection object.
+    """
+    super().__init__(
+        binary_graph_path=_FULL_GRAPH_FILE_PATH)
+
+  # input_video is the input_stream name from the graph
+  def process(self, image: np.ndarray) -> NamedTuple:
+    return super().process(input_data={'input_video': image})
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/python/solutions/ovms_holistic_tracking.py mediapipe/mediapipe/python/solutions/ovms_holistic_tracking.py
--- ../mediapipe/mediapipe/python/solutions/ovms_holistic_tracking.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/python/solutions/ovms_holistic_tracking.py	2023-11-27 13:34:35.519907171 +0100
@@ -0,0 +1,44 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+import numpy as np
+from typing import NamedTuple
+
+from mediapipe.calculators.ovms import openvinoinferencecalculator_pb2
+from mediapipe.calculators.ovms import openvinomodelserversessioncalculator_pb2
+from mediapipe.python.solution_base import SolutionBase
+
+_FULL_GRAPH_FILE_PATH = 'mediapipe/modules/ovms_modules/holistic_tracking_ovms.binarypb'
+
+class OvmsHolisticTracking(SolutionBase):
+  """Ovms Holistic Tracking.
+
+  Ovms Holistic Tracking processes an input image frame returns output image frame
+  with detected objects.
+  """
+  """
+  Oryginal params in desktop example
+  --calculator_graph_config_file /mediapipe/mediapipe/graphs/holistic_tracking/holistic_tracking_cpu.pbtxt
+  --input_video_path=/mediapipe/video.mp4
+  --output_video_path=/mediapipe/output_holistic_ovms.mp4
+  """
+  def __init__(self):
+    """Initializes a Ovms Holistic Tracking object.
+    """
+    super().__init__(
+        binary_graph_path=_FULL_GRAPH_FILE_PATH)
+
+  # input_video is the input_stream name from the graph
+  def process(self, image: np.ndarray) -> NamedTuple:
+    return super().process(input_data={'input_video': image})
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/python/solutions/ovms_object_detection.py mediapipe/mediapipe/python/solutions/ovms_object_detection.py
--- ../mediapipe/mediapipe/python/solutions/ovms_object_detection.py	1970-01-01 01:00:00.000000000 +0100
+++ mediapipe/mediapipe/python/solutions/ovms_object_detection.py	2023-11-27 13:34:35.519907171 +0100
@@ -0,0 +1,46 @@
+# Copyright (c) 2023 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+"""Ovms Object Detection."""
+
+from mediapipe.calculators.ovms import openvinoinferencecalculator_pb2
+from mediapipe.calculators.ovms import openvinomodelserversessioncalculator_pb2
+from mediapipe.python.solution_base import SolutionBase
+
+_FULL_GRAPH_FILE_PATH = 'mediapipe/modules/ovms_modules/object_detection_ovms.binarypb'
+
+class OvmsObjectDetection(SolutionBase):
+  """Ovms Object Detection.
+
+  Ovms Object Detection processes an input video returns output video
+  with detected objects.
+  """
+  """
+  Oryginal params in desktop example
+  --calculator_graph_config_file mediapipe/graphs/object_detection/object_detection_desktop_ovms1_graph.pbtxt
+  --input_side_packets "input_video_path=/mediapipe/mediapipe/examples/desktop/object_detection/test_video.mp4,output_video_path=/mediapipe/tested_video.mp4
+  """
+  def __init__(self,
+              side_inputs=
+              {'input_video_path':'/mediapipe/mediapipe/examples/desktop/object_detection/test_video.mp4',
+              'output_video_path':'/mediapipe/tested_video.mp4'}):
+    """Initializes a Ovms Object Detection object.
+    """
+    super().__init__(
+        binary_graph_path=_FULL_GRAPH_FILE_PATH,
+        side_inputs=side_inputs)
+
+  def process(self):
+    self._graph.wait_until_done()
+    return None
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/tasks/cc/components/processors/proto/detection_postprocessing_graph_options.proto mediapipe/mediapipe/tasks/cc/components/processors/proto/detection_postprocessing_graph_options.proto
--- ../mediapipe/mediapipe/tasks/cc/components/processors/proto/detection_postprocessing_graph_options.proto	2023-10-12 11:41:03.721641527 +0200
+++ mediapipe/mediapipe/tasks/cc/components/processors/proto/detection_postprocessing_graph_options.proto	2023-11-06 15:23:55.770935074 +0100
@@ -27,23 +27,23 @@
 message DetectionPostprocessingGraphOptions {
   // Optional SsdAnchorsCalculatorOptions for models without
   // non-maximum-suppression in tflite model graph.
-  optional mediapipe.SsdAnchorsCalculatorOptions ssd_anchors_options = 1;
+  mediapipe.SsdAnchorsCalculatorOptions ssd_anchors_options = 1;
 
   // Optional TensorsToDetectionsCalculatorOptions for models without
   // non-maximum-suppression in tflite model graph.
-  optional mediapipe.TensorsToDetectionsCalculatorOptions
+  mediapipe.TensorsToDetectionsCalculatorOptions
       tensors_to_detections_options = 2;
 
   // Optional NonMaxSuppressionCalculatorOptions for models without
   // non-maximum-suppression in tflite model graph.
-  optional mediapipe.NonMaxSuppressionCalculatorOptions
+  mediapipe.NonMaxSuppressionCalculatorOptions
       non_max_suppression_options = 3;
 
   // Optional score calibration options for models with non-maximum-suppression
   // in tflite model graph.
-  optional ScoreCalibrationCalculatorOptions score_calibration_options = 4;
+  ScoreCalibrationCalculatorOptions score_calibration_options = 4;
 
   // Optional detection label id to text calculator options.
-  optional mediapipe.DetectionLabelIdToTextCalculatorOptions
+  mediapipe.DetectionLabelIdToTextCalculatorOptions
       detection_label_ids_to_text_options = 5;
 }
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/mediapipe/util/tflite/operations/max_pool_argmax.cc mediapipe/mediapipe/util/tflite/operations/max_pool_argmax.cc
--- ../mediapipe/mediapipe/util/tflite/operations/max_pool_argmax.cc	2023-10-12 11:40:51.720405884 +0200
+++ mediapipe/mediapipe/util/tflite/operations/max_pool_argmax.cc	2023-10-11 16:18:56.037702503 +0200
@@ -156,6 +156,7 @@
   output_size->data[3] = channels_out;
   TfLiteIntArray* indices_size = TfLiteIntArrayCopy(output_size);
   if (context->ResizeTensor(context, output, output_size) != kTfLiteOk) {
+    TfLiteIntArrayFree(indices_size);
     return kTfLiteError;
   }
   if (context->ResizeTensor(context, indices, indices_size) != kTfLiteOk) {
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/setup_opencv.sh mediapipe/setup_opencv.sh
--- ../mediapipe/setup_opencv.sh	2023-10-12 11:40:46.943914096 +0200
+++ mediapipe/setup_opencv.sh	2023-07-06 12:59:40.890542030 +0200
@@ -39,13 +39,13 @@
   then
     echo "Installing OpenCV from source"
     if [[ -x "$(command -v apt)" ]]; then
-      sudo apt update && sudo apt install build-essential git
-      sudo apt install cmake ffmpeg libavformat-dev libdc1394-22-dev libgtk2.0-dev \
+      apt update && apt install -y build-essential git
+      apt install -y cmake ffmpeg libavformat-dev libdc1394-22-dev libgtk2.0-dev \
                        libjpeg-dev libpng-dev libswscale-dev libtbb2 libtbb-dev \
                        libtiff-dev
     elif [[ -x "$(command -v dnf)" ]]; then
-      sudo dnf update && sudo dnf install cmake gcc gcc-c git
-      sudo dnf install ffmpeg-devel libdc1394-devel gtk2-devel \
+      dnf update && dnf install -y cmake gcc gcc-c git
+      dnf install ffmpeg-devel libdc1394-devel gtk2-devel \
                        libjpeg-turbo-devel libpng-devel tbb-devel \
                        libtiff-devel
     fi
@@ -56,11 +56,12 @@
     git clone https://github.com/opencv/opencv.git
     mkdir opencv/release
     cd opencv_contrib
-    git checkout 3.4
+    git checkout 4.7.0
     cd ../opencv
-    git checkout 3.4
+    git checkout 4.7.0
     cd release
     cmake .. -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=/usr/local \
+          -DBUILD_LIST=core,improc,imgcodecs,calib3d,features2d,highgui,imgproc,video,videoio,optflow \
           -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_opencv_ts=OFF \
           -DOPENCV_EXTRA_MODULES_PATH=/tmp/build_opencv/opencv_contrib/modules \
           -DBUILD_opencv_aruco=OFF -DBUILD_opencv_bgsegm=OFF -DBUILD_opencv_bioinspired=OFF \
@@ -75,14 +76,14 @@
           -DCV_ENABLE_INTRINSICS=ON -DWITH_EIGEN=ON -DWITH_PTHREADS=ON -DWITH_PTHREADS_PF=ON \
           -DWITH_JPEG=ON -DWITH_PNG=ON -DWITH_TIFF=ON
     make -j 16
-    sudo make install
+    make install
     rm -rf /tmp/build_opencv
     echo "OpenCV has been built. You can find the header files and libraries in /usr/local/include/opencv2/ and /usr/local/lib"
 
     # https://github.com/cggos/dip_cvqt/issues/1#issuecomment-284103343
-    sudo touch /etc/ld.so.conf.d/mp_opencv.conf
-    sudo bash -c  "echo /usr/local/lib >> /etc/ld.so.conf.d/mp_opencv.conf"
-    sudo ldconfig -v
+    touch /etc/ld.so.conf.d/mp_opencv.conf
+    bash -c  "echo /usr/local/lib >> /etc/ld.so.conf.d/mp_opencv.conf"
+    ldconfig -v
 fi
 
 # Modify the build file.
@@ -93,4 +94,5 @@
 path_line=$((linux_opencv_config + 2))
 sed -i "$path_line d" $workspace_file
 sed -i "$path_line i\    path = \"/usr/local\"," $workspace_file
+cat $opencv_build_file
 echo "Done"
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/setup.py mediapipe/setup.py
--- ../mediapipe/setup.py	2023-10-12 11:40:51.732407119 +0200
+++ mediapipe/setup.py	2023-11-27 13:34:35.523907575 +0100
@@ -30,8 +30,8 @@
 from setuptools.command import build_py
 from setuptools.command import install
 
-__version__ = 'dev'
 MP_DISABLE_GPU = os.environ.get('MEDIAPIPE_DISABLE_GPU') != '0'
+__version__ = '1.0'
 IS_WINDOWS = (platform.system() == 'Windows')
 IS_MAC = (platform.system() == 'Darwin')
 MP_ROOT_PATH = os.path.dirname(os.path.abspath(__file__))
@@ -48,6 +48,7 @@
 ]
 GPU_OPTIONS = GPU_OPTIONS_DISBALED if MP_DISABLE_GPU else GPU_OPTIONS_ENBALED
 
+OVMS_OPTIONS = ['--define=MEDIAPIPE_DISABLE=1 --define=PYTHON_DISABLE=1 --cxxopt=-DPYTHON_DISABLE=1 --cxxopt=-DMEDIAPIPE_DISABLE=1']
 
 def _normalize_path(path):
   return path.replace('\\', '/') if IS_WINDOWS else path
@@ -131,6 +132,8 @@
   # Save the original mediapipe/__init__.py file.
   shutil.copyfile(MP_DIR_INIT_PY, _get_backup_file(MP_DIR_INIT_PY))
   mp_dir_init_file = open(MP_DIR_INIT_PY, 'a')
+  # Removes licence but clears contents so that it doesnt get messed up with every build
+  mp_dir_init_file.truncate(0);
   mp_dir_init_file.writelines([
       '\n', 'from mediapipe.python import *\n',
       'import mediapipe.python.solutions as solutions \n',
@@ -272,7 +275,10 @@
         'hand_landmark/hand_landmark_tracking_cpu',
         'holistic_landmark/holistic_landmark_cpu', 'objectron/objectron_cpu',
         'pose_landmark/pose_landmark_cpu',
-        'selfie_segmentation/selfie_segmentation_cpu'
+        'selfie_segmentation/selfie_segmentation_cpu',
+        'ovms_modules/object_detection_ovms',
+        'ovms_modules/holistic_tracking_ovms',
+        'ovms_modules/face_detection_ovms',
     ]
     for elem in binary_graphs:
       binary_graph = os.path.join('mediapipe/modules/', elem)
@@ -300,7 +306,7 @@
         '--copt=-DNDEBUG',
         '--action_env=PYTHON_BIN_PATH=' + _normalize_path(sys.executable),
         binary_graph_target,
-    ] + GPU_OPTIONS
+    ] + GPU_OPTIONS + OVMS_OPTIONS
 
     if not self.link_opencv and not IS_WINDOWS:
       bazel_command.append('--define=OPENCV=source')
@@ -326,7 +332,7 @@
           '--compilation_mode=opt',
           '--action_env=PYTHON_BIN_PATH=' + _normalize_path(sys.executable),
           '//mediapipe/tasks/metadata:' + target,
-      ] + GPU_OPTIONS
+      ] + GPU_OPTIONS + OVMS_OPTIONS
 
       _invoke_shell_command(bazel_command)
       _copy_to_build_lib_dir(
@@ -413,7 +419,7 @@
         '--copt=-DNDEBUG',
         '--action_env=PYTHON_BIN_PATH=' + _normalize_path(sys.executable),
         str(ext.bazel_target + '.so'),
-    ] + GPU_OPTIONS
+    ] + GPU_OPTIONS + OVMS_OPTIONS
 
     if extra_args:
       bazel_command += extra_args
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/third_party/BUILD mediapipe/third_party/BUILD
--- ../mediapipe/third_party/BUILD	2023-11-27 15:50:32.073992573 +0100
+++ mediapipe/third_party/BUILD	2023-11-14 14:59:44.111495869 +0100
@@ -150,7 +150,7 @@
 # still only builds the shared libraries, so we have to choose one or the
 # other. We build shared libraries by default, but this variable can be used
 # to switch to static libraries.
-OPENCV_SHARED_LIBS = True
+OPENCV_SHARED_LIBS = False
 
 OPENCV_SO_VERSION = "3.4"
 
diff -uraN -x '*.git*' -x 'openvino*' -x '*.png' -x 'modelapi*' -x '*.groovy' -x 'Dockerfile.*' -x '*.md' -x '*.json' -x WORKSPACE -x '*.xml' -x .bazelrc -x Makefile ../mediapipe/third_party/opencv_linux.BUILD mediapipe/third_party/opencv_linux.BUILD
--- ../mediapipe/third_party/opencv_linux.BUILD	2023-10-12 11:40:46.943914096 +0200
+++ mediapipe/third_party/opencv_linux.BUILD	2023-07-06 13:15:00.247820232 +0200
@@ -18,16 +18,18 @@
         #"include/aarch64-linux-gnu/opencv4/opencv2/cvconfig.h",
         #"include/arm-linux-gnueabihf/opencv4/opencv2/cvconfig.h",
         #"include/x86_64-linux-gnu/opencv4/opencv2/cvconfig.h",
-        #"include/opencv4/opencv2/**/*.h*",
+        "include/opencv4/opencv2/**/*.h*",
+        "include/opencv4/opencv2/*.h*",
     ]),
     includes = [
         # For OpenCV 4.x
         #"include/aarch64-linux-gnu/opencv4/",
         #"include/arm-linux-gnueabihf/opencv4/",
         #"include/x86_64-linux-gnu/opencv4/",
-        #"include/opencv4/",
+        "include/opencv4/",
     ],
     linkopts = [
+        "-L/usr/local/lib",
         "-l:libopencv_core.so",
         "-l:libopencv_calib3d.so",
         "-l:libopencv_features2d.so",
@@ -36,6 +38,7 @@
         "-l:libopencv_imgproc.so",
         "-l:libopencv_video.so",
         "-l:libopencv_videoio.so",
+        "-l:libopencv_optflow.so",
     ],
     visibility = ["//visibility:public"],
 )
